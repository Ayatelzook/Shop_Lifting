{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2ebba97-6f8d-401a-8fbd-7f891068852a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv3D, MaxPooling3D, Flatten, Dense, Dropout, BatchNormalization, LSTM, TimeDistributed\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, recall_score, f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef3260f5-644e-46f8-b561-667c96a495d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "IMG_SIZE = 128\n",
    "NUM_FRAMES = 30\n",
    "CHANNELS = 3\n",
    "NUM_CLASSES = 2\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 0.000005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c4cd6ad-c948-4426-9e4c-28d9b9835242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load and preprocess video using optical flow\n",
    "def load_video(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    frame_idxs = np.linspace(0, max(total_frames-1, 1), NUM_FRAMES).astype(int)\n",
    "\n",
    "    prev_gray = None\n",
    "    for idx in frame_idxs:\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame = cv2.resize(frame, (IMG_SIZE, IMG_SIZE))\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        if prev_gray is not None:\n",
    "            flow = cv2.calcOpticalFlowFarneback(prev_gray, gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "            mag, _ = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "\n",
    "            # Normalize and reshape mag to match RGB shape\n",
    "            mag = cv2.normalize(mag, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
    "            mag = np.expand_dims(mag, axis=-1)  # Convert to (H, W, 1)\n",
    "            frame = np.concatenate((frame, mag), axis=-1)  # Now (H, W, 4)\n",
    "\n",
    "        else:\n",
    "            frame = np.dstack((frame, np.zeros((IMG_SIZE, IMG_SIZE, 1))))  # Add empty motion channel\n",
    "\n",
    "        prev_gray = gray\n",
    "        frames.append(frame)\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    while len(frames) < NUM_FRAMES:\n",
    "        frames.append(frames[-1])  # Pad with last frame\n",
    "\n",
    "    return np.array(frames, dtype=np.float32) / 255.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91c5e06e-e62e-41e7-8fd1-2c514980c817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "def load_dataset(data_dir):\n",
    "    X, y = [], []\n",
    "    labels = {'non shop lifters': 0, 'shop lifters': 1}\n",
    "\n",
    "    for label in labels.keys():\n",
    "        folder_path = os.path.join(data_dir, label)\n",
    "        for file in os.listdir(folder_path):\n",
    "            if file.endswith(\".mp4\"):\n",
    "                video_path = os.path.join(folder_path, file)\n",
    "                X.append(load_video(video_path))\n",
    "                y.append(labels[label])\n",
    "\n",
    "    return np.array(X), np.array(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8876c85e-4cf1-4cee-bdfc-5d56cc68bf35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "data_dir = r\"C:\\Users\\Eman\\Downloads\\Shop DataSet\\Shop DataSet\"\n",
    "X, y = load_dataset(data_dir)\n",
    "\n",
    "# Shuffle dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices((X, y)).shuffle(len(X), reshuffle_each_iteration=True)\n",
    "\n",
    "# Split dataset (80% train, 20% test)\n",
    "train_size = int(0.8 * len(X))\n",
    "train_dataset = dataset.take(train_size).batch(BATCH_SIZE)\n",
    "test_dataset = dataset.skip(train_size).batch(BATCH_SIZE)\n",
    "\n",
    "# Handle class imbalance\n",
    "class_weights = compute_class_weight(\"balanced\", classes=np.unique(y), y=y)\n",
    "class_weight_dict = {i: class_weights[i] for i in range(NUM_CLASSES)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b39c7b6a-4aaf-4b3e-a25f-802b7504b1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Hybrid 3D CNN + LSTM model\n",
    "def build_model():\n",
    "    model = Sequential([\n",
    "        Conv3D(32, kernel_size=(3, 3, 3), activation='relu', input_shape=(NUM_FRAMES, IMG_SIZE, IMG_SIZE, CHANNELS + 1)),\n",
    "        MaxPooling3D(pool_size=(2, 2, 2)),\n",
    "        BatchNormalization(),\n",
    "\n",
    "        Conv3D(64, kernel_size=(3, 3, 3), activation='relu'),\n",
    "        MaxPooling3D(pool_size=(2, 2, 2)),\n",
    "        BatchNormalization(),\n",
    "\n",
    "        Conv3D(128, kernel_size=(3, 3, 3), activation='relu'),\n",
    "        MaxPooling3D(pool_size=(2, 2, 2)),\n",
    "        BatchNormalization(),\n",
    "\n",
    "        TimeDistributed(Flatten()),  # Preserve temporal features\n",
    "        LSTM(64, return_sequences=False),  # Temporal learning\n",
    "\n",
    "        Dense(256, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(NUM_CLASSES, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=LEARNING_RATE), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d376e730-b8b0-4883-bcc7-ebf173cb323d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Eman\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ conv3d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv3D</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)    │           <span style=\"color: #00af00; text-decoration-color: #00af00\">3,488</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling3d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling3D</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)      │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv3d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv3D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">61</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">61</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">55,360</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling3d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling3D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_1                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv3d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv3D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │         <span style=\"color: #00af00; text-decoration-color: #00af00\">221,312</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling3d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling3D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_2                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ time_distributed (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25088</span>)            │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │       <span style=\"color: #00af00; text-decoration-color: #00af00\">6,439,168</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">16,640</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">514</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ conv3d (\u001b[38;5;33mConv3D\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m126\u001b[0m, \u001b[38;5;34m126\u001b[0m, \u001b[38;5;34m32\u001b[0m)    │           \u001b[38;5;34m3,488\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling3d (\u001b[38;5;33mMaxPooling3D\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m32\u001b[0m)      │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m32\u001b[0m)      │             \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv3d_1 (\u001b[38;5;33mConv3D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m61\u001b[0m, \u001b[38;5;34m61\u001b[0m, \u001b[38;5;34m64\u001b[0m)      │          \u001b[38;5;34m55,360\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling3d_1 (\u001b[38;5;33mMaxPooling3D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_1                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │             \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv3d_2 (\u001b[38;5;33mConv3D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │         \u001b[38;5;34m221,312\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling3d_2 (\u001b[38;5;33mMaxPooling3D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_2                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │             \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ time_distributed (\u001b[38;5;33mTimeDistributed\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m25088\u001b[0m)            │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │       \u001b[38;5;34m6,439,168\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │          \u001b[38;5;34m16,640\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                   │             \u001b[38;5;34m514\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,737,378</span> (25.70 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m6,737,378\u001b[0m (25.70 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,736,930</span> (25.70 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m6,736,930\u001b[0m (25.70 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">448</span> (1.75 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m448\u001b[0m (1.75 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 1s/step - accuracy: 0.6305 - loss: 0.6595 - val_accuracy: 0.5246 - val_loss: 0.6916\n",
      "Epoch 2/10\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 1s/step - accuracy: 0.8371 - loss: 0.4987 - val_accuracy: 0.4836 - val_loss: 0.7075\n",
      "Epoch 3/10\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 1s/step - accuracy: 0.9111 - loss: 0.3498 - val_accuracy: 0.4754 - val_loss: 0.7064\n",
      "Epoch 4/10\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 1s/step - accuracy: 0.9219 - loss: 0.2792 - val_accuracy: 0.5492 - val_loss: 0.6686\n",
      "Epoch 5/10\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 1s/step - accuracy: 0.9417 - loss: 0.2106 - val_accuracy: 0.4590 - val_loss: 0.6599\n",
      "Epoch 6/10\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 1s/step - accuracy: 0.9735 - loss: 0.1581 - val_accuracy: 0.7377 - val_loss: 0.5369\n",
      "Epoch 7/10\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 1s/step - accuracy: 0.9747 - loss: 0.1538 - val_accuracy: 0.9344 - val_loss: 0.3978\n",
      "Epoch 8/10\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 1s/step - accuracy: 0.9703 - loss: 0.1313 - val_accuracy: 0.9918 - val_loss: 0.2571\n",
      "Epoch 9/10\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 1s/step - accuracy: 0.9887 - loss: 0.0981 - val_accuracy: 0.9836 - val_loss: 0.1673\n",
      "Epoch 10/10\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 1s/step - accuracy: 0.9892 - loss: 0.0956 - val_accuracy: 1.0000 - val_loss: 0.1017\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x202c850fbc0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model = build_model()\n",
    "model.summary()\n",
    "model.fit(train_dataset, epochs=EPOCHS, validation_data=test_dataset, class_weight=class_weight_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cce2a321-45cf-4c7f-861e-c45c69ca6e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "model.save(\"shoplifting_3dcnn_lstm.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "db0fae14-9be6-4217-b53f-665b45b596bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 312ms/step - accuracy: 0.9750 - loss: 0.0930\n",
      "Test Loss: 0.1024\n",
      "Test Accuracy: 0.9672\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(test_dataset)\n",
    "print(f\"Test Loss: {loss:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eb7dc3ce-a3fb-4e7b-b9aa-5abbfed3a431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 331ms/step\n",
      "Classification Report:\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "Non-Shoplifting       0.52      0.53      0.52        64\n",
      "    Shoplifting       0.46      0.45      0.46        58\n",
      "\n",
      "       accuracy                           0.49       122\n",
      "      macro avg       0.49      0.49      0.49       122\n",
      "   weighted avg       0.49      0.49      0.49       122\n",
      "\n",
      "Confusion Matrix:\n",
      "[[34 30]\n",
      " [32 26]]\n",
      "Recall: 0.4918\n",
      "F1-score: 0.4913\n"
     ]
    }
   ],
   "source": [
    "# Get true labels and predictions\n",
    "y_true = np.concatenate([y for _, y in test_dataset], axis=0)\n",
    "y_pred = np.argmax(model.predict(test_dataset), axis=1)\n",
    "\n",
    "# Compute recall and F1-score\n",
    "recall = recall_score(y_true, y_pred, average='weighted')\n",
    "f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=['Non-Shoplifting', 'Shoplifting']))\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_true, y_pred))\n",
    "\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "88598932-ab6d-4f14-910f-4512f7a180e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 472ms/step\n",
      "Predicted Class: Shoplifting\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load the trained model\n",
    "model = tf.keras.models.load_model(\"shoplifting_3dcnn_lstm.h5\")\n",
    "\n",
    "# Constants\n",
    "IMG_SIZE = 128\n",
    "NUM_FRAMES = 30\n",
    "CHANNELS = 3\n",
    "\n",
    "# Function to preprocess a new video (same as training)\n",
    "def preprocess_video(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    frame_idxs = np.linspace(0, max(total_frames-1, 1), NUM_FRAMES).astype(int)\n",
    "\n",
    "    prev_gray = None\n",
    "    for idx in frame_idxs:\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame = cv2.resize(frame, (IMG_SIZE, IMG_SIZE))\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        if prev_gray is not None:\n",
    "            flow = cv2.calcOpticalFlowFarneback(prev_gray, gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "            mag, _ = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "            mag = cv2.normalize(mag, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
    "            mag = np.expand_dims(mag, axis=-1)  \n",
    "            frame = np.concatenate((frame, mag), axis=-1)  \n",
    "        else:\n",
    "            frame = np.dstack((frame, np.zeros((IMG_SIZE, IMG_SIZE, 1)))) \n",
    "\n",
    "        prev_gray = gray\n",
    "        frames.append(frame)\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    while len(frames) < NUM_FRAMES:\n",
    "        frames.append(frames[-1])  \n",
    "\n",
    "    return np.array(frames, dtype=np.float32) / 255.0  \n",
    "\n",
    "# Path to the test video\n",
    "test_video_path = r\"C:\\Users\\Eman\\Desktop\\task_6\\Shop DataSet\\shop lifters\\videppppsss_30.mp4\"\n",
    "\n",
    "# Preprocess the test video\n",
    "test_video = preprocess_video(test_video_path)\n",
    "\n",
    "# Reshape for model input\n",
    "test_video = np.expand_dims(test_video, axis=0)  # Shape: (1, NUM_FRAMES, IMG_SIZE, IMG_SIZE, 4)\n",
    "\n",
    "# Make prediction\n",
    "prediction = model.predict(test_video)\n",
    "predicted_class = np.argmax(prediction)\n",
    "\n",
    "# Define class labels\n",
    "class_labels = [\"Non-Shoplifting\", \"Shoplifting\"]\n",
    "\n",
    "# Print the result\n",
    "print(f\"Predicted Class: {class_labels[predicted_class]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9b4d4d66-2604-475a-b4ad-fec376f4672c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 452ms/step\n",
      "Predicted Class: Non-Shoplifting\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load the trained model\n",
    "model = tf.keras.models.load_model(\"shoplifting_3dcnn_lstm.h5\")\n",
    "\n",
    "# Constants\n",
    "IMG_SIZE = 128\n",
    "NUM_FRAMES = 30\n",
    "CHANNELS = 3\n",
    "\n",
    "# Function to preprocess a new video (same as training)\n",
    "def preprocess_video(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    frame_idxs = np.linspace(0, max(total_frames-1, 1), NUM_FRAMES).astype(int)\n",
    "\n",
    "    prev_gray = None\n",
    "    for idx in frame_idxs:\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame = cv2.resize(frame, (IMG_SIZE, IMG_SIZE))\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        if prev_gray is not None:\n",
    "            flow = cv2.calcOpticalFlowFarneback(prev_gray, gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "            mag, _ = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "            mag = cv2.normalize(mag, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
    "            mag = np.expand_dims(mag, axis=-1)  \n",
    "            frame = np.concatenate((frame, mag), axis=-1)  \n",
    "        else:\n",
    "            frame = np.dstack((frame, np.zeros((IMG_SIZE, IMG_SIZE, 1)))) \n",
    "\n",
    "        prev_gray = gray\n",
    "        frames.append(frame)\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    while len(frames) < NUM_FRAMES:\n",
    "        frames.append(frames[-1])  \n",
    "\n",
    "    return np.array(frames, dtype=np.float32) / 255.0  \n",
    "\n",
    "# Path to the test video\n",
    "test_video_path = r\"C:\\Users\\Eman\\Desktop\\task_6\\Shop DataSet\\non shop lifters\\shop_lifter_n_217.mp4\"\n",
    "\n",
    "# Preprocess the test video\n",
    "test_video = preprocess_video(test_video_path)\n",
    "\n",
    "# Reshape for model input\n",
    "test_video = np.expand_dims(test_video, axis=0)  # Shape: (1, NUM_FRAMES, IMG_SIZE, IMG_SIZE, 4)\n",
    "\n",
    "# Make prediction\n",
    "prediction = model.predict(test_video)\n",
    "predicted_class = np.argmax(prediction)\n",
    "\n",
    "# Define class labels\n",
    "class_labels = [\"Non-Shoplifting\", \"Shoplifting\"]\n",
    "\n",
    "# Print the result\n",
    "print(f\"Predicted Class: {class_labels[predicted_class]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef50bd51-5653-4eda-8f4d-d6353ce70464",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load the trained model\n",
    "model = tf.keras.models.load_model(\"shoplifting_3dcnn_lstm.h5\")\n",
    "\n",
    "# Constants\n",
    "IMG_SIZE = 128\n",
    "NUM_FRAMES = 30\n",
    "CHANNELS = 3\n",
    "\n",
    "# Function to preprocess a new video (same as training)\n",
    "def preprocess_video(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    frame_idxs = np.linspace(0, max(total_frames-1, 1), NUM_FRAMES).astype(int)\n",
    "\n",
    "    prev_gray = None\n",
    "    for idx in frame_idxs:\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame = cv2.resize(frame, (IMG_SIZE, IMG_SIZE))\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        if prev_gray is not None:\n",
    "            flow = cv2.calcOpticalFlowFarneback(prev_gray, gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "            mag, _ = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "            mag = cv2.normalize(mag, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
    "            mag = np.expand_dims(mag, axis=-1)  \n",
    "            frame = np.concatenate((frame, mag), axis=-1)  \n",
    "        else:\n",
    "            frame = np.dstack((frame, np.zeros((IMG_SIZE, IMG_SIZE, 1)))) \n",
    "\n",
    "        prev_gray = gray\n",
    "        frames.append(frame)\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    while len(frames) < NUM_FRAMES:\n",
    "        frames.append(frames[-1])  \n",
    "\n",
    "    return np.array(frames, dtype=np.float32) / 255.0  \n",
    "\n",
    "# Path to the test video\n",
    "test_video_path = r\"C:\\Users\\Eman\\Desktop\\task_6\\Shop DataSet\\shop lifters\\shop_lifter_3.mp4\"\n",
    "\n",
    "# Preprocess the test video\n",
    "test_video = preprocess_video(test_video_path)\n",
    "\n",
    "# Reshape for model input\n",
    "test_video = np.expand_dims(test_video, axis=0)  # Shape: (1, NUM_FRAMES, IMG_SIZE, IMG_SIZE, 4)\n",
    "\n",
    "# Make prediction\n",
    "prediction = model.predict(test_video)\n",
    "predicted_class = np.argmax(prediction)\n",
    "\n",
    "# Define class labels\n",
    "class_labels = [\"Non-Shoplifting\", \"Shoplifting\"]\n",
    "\n",
    "# Print the result\n",
    "print(f\"Predicted Class: {class_labels[predicted_class]}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
